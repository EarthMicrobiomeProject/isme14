{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a trick to make it transparent if executed on a barnacle node or on my machine with barnacles file system mounted to /media/barnacle\n",
    "import socket\n",
    "ROOT = \"/media/barnacle/\" if socket.gethostname() == 't440s' else \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_root = ROOT + '/projects/emp/'\n",
    "resultsFile = ROOT + '/home/sjanssen/EMP/Task_Observations/observations.tsv'\n",
    "masterMappingFile = path_root + '/00-qiime-maps/merged/emp_qiime_mapping_refined_20160627.tsv'\n",
    "\n",
    "bioms = {\n",
    "    'gg'       :   {'label': 'cr_gg_seqs',\n",
    "                    'biomsummary': path_root + '03-otus/01-closed-ref-greengenes/emp_cr_gg_13_8.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/01-closed-ref-greengenes/emp_cr_gg_13_8.biom'},\n",
    "    'silva'    :   {'label': 'cr_silva_seqs',\n",
    "                    'biomsummary': path_root + '03-otus/01-closed-ref-silva-16S/emp_cr_silva_16S.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/01-closed-ref-silva-16S/emp_cr_silva_16S.biom'},                \n",
    "    'openref'  :   {'label': 'or_gg_seqs',\n",
    "                    'biomsummary': path_root + '03-otus/02-open-ref-greengenes/emp_or.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/02-open-ref-greengenes/emp_or.biom'},                  \n",
    "    'deblur90' :   {'label': 'deblur_90nt',\n",
    "                    'biomsummary': path_root + '03-otus/04-deblur/emp.90.min25.withtax.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/04-deblur/emp.90.min25.withtax.biom'},                  \n",
    "    'deblur100':   {'label': 'deblur_100nt',\n",
    "                    'biomsummary': path_root + '03-otus/04-deblur/emp.100.min25.deblur.withtax.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/04-deblur/emp.100.min25.deblur.withtax.biom'},                  \n",
    "    'deblur150':   {'label': 'deblur_150nt',\n",
    "                    'biomsummary': path_root + '03-otus/04-deblur/emp.150.min25.deblur.withtax.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/04-deblur/emp.150.min25.deblur.withtax.biom'},\n",
    "    'deblur100ot': {'label': 'deblur_100nt_onlytree',\n",
    "                    'biomsummary': path_root + '03-otus/04-deblur/emp.100.min25.deblur.withtax.onlytree.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/04-deblur/emp.100.min25.deblur.withtax.onlytree.biom'},                  \n",
    "    'deblur150ot': {'label': 'deblur_150nt_onlytree',\n",
    "                    'biomsummary': path_root + '03-otus/04-deblur/emp.150.min25.deblur.withtax.onlytree.summary.txt',\n",
    "                    'biomfile':    path_root + '03-otus/04-deblur/emp.150.min25.deblur.withtax.onlytree.biom'},\n",
    "}\n",
    "fasta = {\n",
    "    'split-libraries' : {'label': 'split-libraries',\n",
    "                         'dir': path_root + '01-split-libraries',  \n",
    "                         'fnaFile': 'seqs.fna',          \n",
    "                         'logfile': 'split_library_log.txt'},\n",
    "    'adaptor-clean-up': {'label': 'adaptor-clean-up',\n",
    "                         'dir': path_root + '02-adaptor-clean-up', \n",
    "                         'fnaFile': 'filtered_seqs.fna', \n",
    "                         'filterFnaFile': 'seqs_to_filter.fna',\n",
    "                         'logfile': ''},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/4: parsing summary information about BIOM 8 tables: ........ done.\n",
      "Step 2/4: parsing 97 'split_library_log.txt' files: ................................................................................................. done.\n",
      "Step 3/4: parsing 97 'seqs_to_filter.fna' files: ................................................................................................. done.\n",
      "Step 4/4: add sample names identical to master mapping file '//projects/emp//00-qiime-maps/merged/emp_qiime_mapping_refined_20160627.tsv':  done.\n",
      "\n",
      "Results have been written to '//home/sjanssen/EMP/Task_Observations/observations.tsv'.\n"
     ]
    }
   ],
   "source": [
    "# Gathering all necessary information from the original files themselves is not feasible, because of IO and CPU time.\n",
    "# The following method makes use of additional information that describe the BIOM tables and fasta files.\n",
    "# This is fine, as long as those descriptors are up to date. Otherwise, we are screwed!\n",
    "# These asumptions must hold for proper operations of this script:\n",
    "#     1. each BIOM file is acompanied by a *.summary.txt file which holds the number of counts per sample starting at line 15\n",
    "#     2. each seq.fasta file has its split_library_log.txt which give counts of sequences for the same sample. It can have several runs and counts might be split acros those runs.\n",
    "#     3. to avoid looking into the filered_seqs.fna files we substract the number of sequences to be filtered from the number of sequences per sample of the previous computation. Thus, it is neccessary that we have up-to-date 'seqs_to_filter.fna' files.\n",
    "\n",
    "############ STEP 1/4 ###########\n",
    "print(\"Step 1/4: parsing summary information about BIOM \" + str(len(bioms)) + \" tables: \", end=\"\")\n",
    "o = {}\n",
    "for name in bioms:\n",
    "    print(\".\", end=\"\")\n",
    "    o[name] = pd.read_csv(bioms[name]['biomsummary'], sep=\":\", skiprows=15, index_col=0, names=[\"sampleID\",bioms[name]['label']])\n",
    "    o[name].index = o[name].index.map(str.upper) # use upper case \n",
    "observations = pd.concat([ o[name] for name in o], axis=1, join='outer')\n",
    "print(\" done.\")\n",
    "\n",
    "\n",
    "############ STEP 2/4 ###########\n",
    "field = 'split-libraries'\n",
    "logs = []\n",
    "path = fasta[field]['dir']\n",
    "filename= fasta[field]['logfile']\n",
    "logfiles = !find \"$path\" -name \"$filename\"\n",
    "tmpfile = \"/tmp/sampleIDs\"\n",
    "print(\"Step 2/4: parsing \" + str(len(logfiles)) + \" '\" + filename + \"' files: \", end=\"\")\n",
    "for logfile in logfiles:\n",
    "    print(\".\", end=\"\")\n",
    "    studyID = logfile.split(\"/\")[-2]\n",
    "    #a logfile can contain several entries, thus it is necessary to grep only the sampleID lines in a pre-processing step\n",
    "    !grep \"^$studyID\" \"$logfile\" > \"$tmpfile\"\n",
    "    log = pd.read_csv(tmpfile, sep=\"\\t\", index_col=0, names=['sampleID', fasta[field]['label']])\n",
    "    log.index = log.index.map(str.upper) # use upper case \n",
    "    log = log.reset_index().groupby(log.index).sum() #counts might be spread over several runs, thus we need to sum them here\n",
    "    logs.append(log)\n",
    "logs = pd.concat(logs, axis=0)\n",
    "observations = observations.merge(logs, left_index=True, right_index=True, how=\"outer\")\n",
    "print(\" done.\")\n",
    "\n",
    "\n",
    "############ STEP 3/4 ###########\n",
    "if fasta['split-libraries']['label'] in observations:\n",
    "    field = 'adaptor-clean-up'\n",
    "    path = fasta[field]['dir']\n",
    "    filename = fasta[field]['filterFnaFile']\n",
    "    tmpfile = \"/tmp/sampleIDs\"\n",
    "    filenames_filterSeqs = !find \"$path\" -name \"$filename\"\n",
    "    pdsFiltered = []\n",
    "    print(\"Step 3/4: parsing \" + str(len(filenames_filterSeqs)) + \" '\" + filename + \"' files: \", end=\"\")\n",
    "    for file in filenames_filterSeqs:\n",
    "        print(\".\", end=\"\")\n",
    "        studyID = logfile.split(\"/\")[-2]\n",
    "        !grep \"^>\" \"$file\" | cut -d \"_\" -f 1 | sort | tr -d \">\" | uniq -c | sed \"s/^[ \\t]*//\" > \"$tmpfile\" \n",
    "        nrs = pd.read_csv(tmpfile, sep=\" \", index_col=1, names=[fasta[field]['label'] + \"-remove\", 'sampleID'])\n",
    "        nrs.index = nrs.index.map(str.upper) # use upper case \n",
    "        pdsFiltered.append( nrs )\n",
    "\n",
    "    pdsFiltered = pd.concat(pdsFiltered, axis=0)\n",
    "\n",
    "    #use the counted numbers of sequences that have been filtered to compute the number of remaining sequences\n",
    "    x = observations.merge(pdsFiltered, left_index=True, right_index=True, how=\"outer\")\n",
    "    x.fillna(0, inplace=True)\n",
    "    observations[fasta['adaptor-clean-up']['label']] = observations[fasta['split-libraries']['label']] - x[fasta['adaptor-clean-up']['label']+\"-remove\"]\n",
    "    print(\" done.\")\n",
    "else:\n",
    "    print(\"Field '\" + fasta['split-libraries']['label'] + \"' is missing in the observations table. First compute this field, before computing the number of filtered sequences!\")\n",
    "    \n",
    "\n",
    "############ STEP 4/4 ###########\n",
    "print(\"Step 4/4: add sample names identical to master mapping file '\" + str(masterMappingFile) + \"': \", end=\"\")\n",
    "metadata = pd.read_csv(masterMappingFile, sep=\"\\t\", usecols=[0])\n",
    "metadata['UC'] = metadata['#SampleID'].map(str.upper) # use upper case\n",
    "metadata.set_index(\"UC\", inplace=True)\n",
    "observations = observations.merge(metadata, left_index=True, right_index=True, how=\"inner\")\n",
    "observations.set_index('#SampleID', inplace=True)\n",
    "print(\" done.\")\n",
    "\n",
    "\n",
    "#store results in one file\n",
    "print(\"\\nResults have been written to '\" + resultsFile + \"'.\")\n",
    "observations.to_csv(resultsFile, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute the following command on 'barnacle' and transfer the resulting files somewhere you have access to! If you do this on your local machine ~0.5TB must be transfered via network.\n",
      "\tfor f in `find '/media/barnacle/projects/emp/01-split-libraries/' -name 'seqs.fna' | cut -d '_' -f 1 | sort | uniq -c | tr -d '>' | sed 's/^[ ]*//' >> '/tmp/split-libraries.counts'; done\n",
      "\n",
      "\tfor f in `find '/media/barnacle/projects/emp/02-adaptor-clean-up/' -name 'filtered_seqs.fna' | cut -d '_' -f 1 | sort | uniq -c | tr -d '>' | sed 's/^[ ]*//' >> '/tmp/adaptor-clean-up.counts'; done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the slow method: go through the content of the fasta files and aggregate the header lines. Takes a few hours, even on barnacle!\n",
    "print(\"Execute the following command on 'barnacle' and transfer the resulting files somewhere you have access to! If you do this on your local machine ~0.5TB must be transfered via network.\")\n",
    "for field in ['split-libraries', 'adaptor-clean-up']:\n",
    "    path = fasta[field]['dir']\n",
    "    file = fasta[field]['fnaFile']\n",
    "    tmpFile = '/tmp/' + field + \".counts\"\n",
    "    cmd = \"\\tfor f in `find '\" + path + \"/' -name '\" + file + \"' | cut -d '_' -f 1 | sort | uniq -c | tr -d '>' | sed 's/^[ ]*//' >> '\" + tmpFile + \"'; done\"\n",
    "    print(cmd + \"\\n\")\n",
    "\n",
    "obs = observations\n",
    "for field in ['split-libraries', 'adaptor-clean-up']:\n",
    "    tmpFile = '/media/barnacle/home/sjanssen/' + field + \".counts\"\n",
    "    if os.path.isfile(tmpFile):\n",
    "        x = pd.read_csv(tmpFile, sep=\" \", index_col=1, names=[\"INFASTA_\" + field, \"sampleID\"])\n",
    "        x.index = x.index.map(str.upper) # use upper case\n",
    "        obs = obs.merge(x, left_index=True, right_index=True, how=\"outer\")\n",
    "obs.to_csv(resultsFile, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
